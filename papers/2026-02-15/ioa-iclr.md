# Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation

## メタ情報
- **arXiv ID**: 2602.12172
- **カテゴリ**: cs.AI, cs.CL
- **投稿日**: 2026-02-12
- **会議**: ICLR 2026 採択
- **注目理由**: 🏆 トップ会議採択（ICLR）

## 概要
教育原理に着想を得たLLM知識蒸留フレームワーク。Bloom's Mastery Learning原則とVygotskyの最近接発達領域を統合し、動的な蒸留プロセスを実現。

## 主要貢献
1. **Knowledge Identifier**: 学生モデルの知識欠損を体系的に特定
2. **Knowledge Organizer**: 段階的カリキュラムによる知識配信の組織化
3. **Knowledge Adapter**: 学生モデルの認知容量に合わせた表現適応

## 実験結果
- LLaMA-3.1/3.2、Qwen2.5で大幅改善
- DollyEvalで教師性能の**94.7%**を達成（パラメータ数1/10以下）
- MATHで**19.2%**、HumanEvalで**22.3%**改善（SOTAベースライン比）

## リンク
- 論文: https://arxiv.org/abs/2602.12172
