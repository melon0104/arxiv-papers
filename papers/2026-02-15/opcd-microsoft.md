# On-Policy Context Distillation for Language Models

## メタ情報
- **arXiv ID**: 2602.12275
- **カテゴリ**: cs.CL
- **投稿日**: 2026-02-12
- **機関**: Microsoft Research
- **著者**: Furu Wei他（Microsoft）
- **注目理由**: 🏢 Microsoft Research

## 概要
On-Policy Context Distillation (OPCD) - コンテキスト蒸留とオンポリシー蒸留を橋渡しするフレームワーク。学生モデルが自己生成した軌跡上で、コンテキスト条件付き教師に対する逆KLダイバージェンスを最小化。

## 主要貢献
1. **経験的知識蒸留**: 過去の解軌跡から転移可能な知識を抽出・統合
2. **システムプロンプト蒸留**: 最適化されたプロンプトに埋め込まれた有益な振る舞いを内在化
3. **クロスサイズ蒸留**: 小さい学生モデルが大きい教師から経験的知識を内在化

## 実験結果
- 数学的推論、テキストゲーム、ドメイン固有タスクでベースライン手法を一貫して上回る
- 分布外能力を維持しながら高いタスク精度を達成

## リンク
- 論文: https://arxiv.org/abs/2602.12275
