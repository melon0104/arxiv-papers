# To 2:4 Sparsity and Beyond: Neuron-level Activation Function to Accelerate LLM Pre-Training

**arXiv**: [2602.06183](https://arxiv.org/abs/2602.06183)  
**日付**: 2026-02-09  
**分野**: cs.LG（機械学習）  
**機関**: Meta

## 概要

ハードウェア加速スパース性（2:4スパース性）を活用してLLM事前学習を高速化。FFNの全行列乗算を加速し、重みには2:4スパース性、活性化にはv:n:m (Venom)スパース性を適用。

## 技術的ポイント

- **2:4スパース性**: NVIDIA A100以降のハードウェア加速を活用
- **Venomスパース性**: 活性化に対するv:n:mスパース性
- **段階的訓練**: スパース訓練ステップ＋最終段階でデンス訓練
- **アーキテクチャ横断**: 量子化やMoEと直交する最適化

## 結果

- **訓練高速化**: エンドツーエンドで**1.4〜1.7倍**高速化
- **品質維持**: ベンチマークで同等の性能
- **汎用性**: 標準的な最適化技術と組み合わせ可能

## 選定理由

- **有名研究機関**: Meta
- ハードウェアアウェアなLLM訓練効率化の実用的手法
