# STAPO: Stabilizing RL for LLMs by Silencing Rare Spurious Tokens

**arXiv ID:** 2602.15620  
**Published:** 2026-02-17  
**Categories:** cs.CL, cs.AI

## Authors
Shiqi Liu, Zeyu He, Guojian Zhan, Letian Tao, Zhilong Zheng, Jiang Wu, Yinuo Wang, Yang Guan, Kehua Sheng, Bo Zhang, Keqiang Li, Jingliang Duan, Shengbo Eben Li

## Summary
LLMの強化学習ファインチューニングにおける学習不安定性問題を解決。希少なSpurious Tokensが引き起こす勾配異常を特定し、選択的マスキングで安定化。

## Key Contributions
- **理論的発見**: トークン単位のポリシー勾配の大きさがトークン確率・局所エントロピーと負の相関
- **Spurious Tokens特定**: 約0.01%の稀なトークンが学習不安定性の原因
- **STAPO**: 異常な勾配更新を選択的にマスクし、有効トークンで損失を再正規化

## Technical Approach
1. トークン確率と勾配の関係を理論的に導出
2. Spurious Tokens（正解応答内の非本質的トークン）を特定
3. これらのトークンの勾配更新をマスク
4. 有効トークン上で損失を再正規化

## Results
Qwen 1.7B, 8B, 14Bモデルで6つの数学推論ベンチマークで評価：
- GRPOに対して平均7.13%の性能向上
- 20-Entropy、JustRLも上回る
- エントロピー安定性の大幅改善
- 後期学習崩壊の防止

## Significance
LLMのRL学習における根本的な不安定性メカニズムを解明。実用的なソリューションにより、大規模モデルの安定した強化学習が可能に。
