# LoRWeB: Spanning the Visual Analogy Space with a Weight Basis of LoRAs

**arXiv ID:** 2602.15727  
**Published:** 2026-02-17  
**Categories:** cs.CV, cs.AI, cs.LG  
**Institution:** NVIDIA Research  
**Code:** https://research.nvidia.com/labs/par/lorweb

## Authors
Hila Manor, **Rinon Gal**, Haggai Maron, Tomer Michaeli, **Gal Chechik**

## Summary
視覚的アナロジー学習（a:a'::b:b'の形式での画像変換）を改善するLoRWeB。学習済みLoRA基底の動的合成で、推論時に各タスクに特化したモデルを構築。

## Key Contributions
- **学習可能なLoRA基底**: 様々な視覚変換をスパンするLoRAモジュールの基底セット
- **動的基底選択**: 入力アナロジーペアに基づいてLoRAを選択・重み付けする軽量エンコーダ
- **推論時特化**: 各アナロジータスクに対して「LoRA空間内の点」を選択
- **汎化能力向上**: 未知の視覚変換への大幅な汎化改善

## Technical Approach
1. 視覚変換プリミティブを表現するLoRA基底を学習
2. 軽量エンコーダがアナロジーペアから基底重みを予測
3. 推論時に動的にLoRAを合成
4. 固定適応モジュールの限界を克服

## Results
- 視覚アナロジータスクでSOTA達成
- 未知の視覚変換への汎化が大幅向上
- LoRA基底分解の有効性を実証

## Significance
NVIDIA Research（Rinon Gal, Gal Chechik）からの重要な貢献。テキスト記述が困難な複雑な視覚変換の指定を可能にし、柔軟な画像編集への新しいアプローチを提示。
