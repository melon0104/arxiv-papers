# ColBERT-Zero: To Pre-train Or Not To Pre-train ColBERT models

**arXiv**: [2602.16609](https://arxiv.org/abs/2602.16609)  
**カテゴリ**: cs.CL, cs.IR  
**投稿日**: 2026-02-18  
**コード**: 公開予定

## 著者
- Antoine Chaffin, Luca Arnaboldi, Amélie Chatelain, Florent Krzakala

## 概要

マルチベクトルモデル（ColBERTなど）のプリトレーニングを体系的に研究し、**公開データのみでSOTA達成**。

### 主要な発見
1. **大規模プリトレーニングの効果**: マルチベクトル専用の大規模事前学習がはるかに強力なモデルを生成
2. **ColBERT-Zero**: 公開データのみで訓練、**GTE-ModernColBERT**および**GTE-ModernBERT**（クローズドデータ使用）を上回る
3. **効率的な代替**: 小さなKDステップだけでは不十分だが、事前に教師ありステップを追加することで、最もコストの高い教師なしフェーズをスキップ可能

### 実験結果
- このサイズのモデルで**新SOTA**を達成
- ファインチューニングとプリトレーニングのセットアップの整合が重要

## なぜ重要か
- ColBERTの事前学習に関する初の体系的研究
- 公開データのみでSOTA達成（再現可能性）
- チェックポイントとコードを公開予定
