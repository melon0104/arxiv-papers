# Arcee Trinity Large Technical Report

- **arXiv ID**: 2602.17004
- **カテゴリ**: cs.LG, cs.CL
- **投稿日**: 2026-02-19
- **リンク**: https://arxiv.org/abs/2602.17004
- **コード**: https://huggingface.co/arcee-ai

## 概要

Arcee Trinity Largeは、総パラメータ400B、トークンあたり13Bアクティベートのスパース Mixture-of-Experts モデル。Trinity Nano（総6B、1Bアクティベート）とTrinity Mini（総26B、3Bアクティベート）も同時に報告。

モダンアーキテクチャには、インターリーブされたローカル・グローバルアテンション、ゲート付きアテンション、深さスケール付きサンドイッチノルム、MoE用シグモイドルーティングが含まれる。Trinity Large向けには、新しいMoEロードバランシング戦略「Soft-clamped Momentum Expert Bias Updates (SMEBU)」を導入。Muonオプティマイザで学習し、3モデルすべてがロススパイクなしで学習完了。Trinity Nano/Miniは10兆トークン、Trinity Largeは17兆トークンで事前学習。

## 注目ポイント

- **大規模MoEモデル**: 400B総パラメータ、17兆トークン学習
- **新規アーキテクチャ要素**: SMEBU、Muonオプティマイザ
- **安定学習**: ロススパイクなし
- **オープンソース**: HuggingFaceで公開
