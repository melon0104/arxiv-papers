---
layout: paper
title: "The Anxiety of Influence: Bloom Filters in Transformer Attention Heads"
date: 2026-02-19
arxiv_id: "2602.17526"
categories: [cs.LG, cs.AI, cs.CL]
github: "https://github.com/pbalogh/anxiety-of-influence"
---

# Bloom Filters in Transformer Attention Heads

**arXiv**: [2602.17526](https://arxiv.org/abs/2602.17526)  
**GitHub**: [pbalogh/anxiety-of-influence](https://github.com/pbalogh/anxiety-of-influence)

## 概要

Transformerの一部のアテンションヘッドが「このトークンは以前文脈に出現したか？」という**メンバーシップテスト**として機能していることを発見。

## 主要な発見

1. **高精度メンバーシップフィルタ**: L0H1/L0H5（GPT-2 small）は180ユニークトークンでも偽陽性率0-4%
2. **古典的Bloom Filter曲線**: L1H11は理論式 p ≈ (1 - e^{-kn/m})^k に R²=1.0 で一致
3. **多解像度システム**: 3つの真のメンバーシップテストヘッドが初期層（0-1）に集中

## 技術詳細

- GPT-2 small/medium/large、Pythia-160Mで検証
- Induction headやprevious-token headとは分類上異なる
- 埋め込み距離に応じて偽陽性率が単調減少

## 一般化能力

- 反復する名前だけでなく任意の反復トークン型に反応
- duplicate-token-only headより43%高い一般化

## 意義

Transformerの内部メカニズムへの新しい解釈可能性の提供。古典的データ構造との驚くべき対応。
