---
layout: paper
title: "Bayesian Optimality of In-Context Learning with Selective State Spaces"
arxiv_id: "2602.17744"
date: 2026-02-24
categories: [cs.LG, cs.CL, math.ST, stat.ML]
---

# Bayesian Optimality of In-Context Learning with Selective State Spaces

## 基本情報
- **arXiv**: [2602.17744](https://arxiv.org/abs/2602.17744)
- **カテゴリ**: cs.LG, cs.CL, math.ST, stat.ML
- **投稿日**: 2026-02-19

## 概要
In-Context Learning (ICL)をベイズ最適順次予測として理解する新原理を提案。Selective SSMがベイズ最適予測器を漸近的に実装することを証明し、勾配降下からの統計的分離を確立。

## 主要な貢献
1. **ICLのベイズ的形式化**: 潜在系列タスクに対するメタ学習としてICLを定式化
2. **収束証明**: Linear Gaussian SSMタスクで、メタ訓練されたSelective SSMが事後予測平均に収束
3. **統計的分離**: 時間的相関ノイズを持つタスクで、ベイズ最適予測器がERM推定器を厳密に上回ることを構築

## 理論的意義
- Transformerは暗黙的ERMを実行するため、Selective SSMがより低い漸近リスクを達成
- ICLを「暗黙的最適化」から「最適推論」として再フレーム
- Selective SSMの効率性を説明し、アーキテクチャ設計の原理的基盤を提供

## 実験
- 合成LG-SSMタスク
- 文字レベルMarkovベンチマーク
- 結果: Selective SSMがベイズ最適リスクに早く収束

## 関連キーワード
In-Context Learning, State Space Models, Bayesian Inference, Mamba, Meta-Learning
