---
layout: paper
title: "Understanding the Fine-Grained Knowledge Capabilities of VLMs"
arxiv_id: "2602.17871"
date: 2026-02-24
categories: [cs.CV, cs.AI, cs.LG, cs.MM]
---

# Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models

## 基本情報
- **arXiv**: [2602.17871](https://arxiv.org/abs/2602.17871)
- **カテゴリ**: cs.CV, cs.AI, cs.LG, cs.MM
- **投稿日**: 2026-02-19

## 概要
Vision-Language Models (VLMs)が多くのVQAベンチマークで進歩する一方、細粒度画像分類では遅れを取る原因を調査。VLMの視覚中心能力向上への洞察を提供。

## 主要な発見
1. **LLMの改善**: より良いLLMはすべてのベンチマークスコアを均等に改善
2. **Vision Encoderの改善**: より良いVision Encoderは細粒度分類性能を不均衡に改善
3. **事前学習段階の重要性**: 事前学習中にLLM重みを解凍することが細粒度性能に重要

## Ablation実験の知見
- Vision EncoderがFine-Grained Knowledge獲得の鍵
- 事前学習段階でのLLM重みの扱いが性能に大きく影響
- Fine-grained分類と一般VQAベンチマークの間の乖離を説明

## 意義
- VLMのFine-Grained視覚理解向上への道筋を提示
- Vision中心能力強化のアーキテクチャ設計指針
- 既存VLMの弱点を体系的に分析

## 関連キーワード
Vision-Language Models, Fine-Grained Classification, Vision Encoder, Multimodal Learning
