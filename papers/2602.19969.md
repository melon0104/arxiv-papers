---
layout: paper
arxiv_id: "2602.19969"
title: "ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting"
authors: "Yuxing Tian, Fengran Mo, Weixu Zhang, Yiyan Qi, Jian-Yun Nie"
categories: "cs.CL cs.AI"
published: "2026-02-23"
venue: "EACL 2026"
github: null
abstract: |
  The strong capabilities of recent Large Language Models (LLMs) have made them highly effective for zero-shot re-ranking task. Attention-based re-ranking methods, which derive relevance scores directly from attention weights, offer an efficient and interpretable alternative to generation-based re-ranking methods.
---

# ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting

## 概要

LLMを用いた**attention-based re-ranking**の改善手法。attention weightsから直接関連度スコアを導出する効率的で解釈可能なアプローチ。

## 主な貢献

- **ReAttn**: attention-based re-ranking向けのpost-hoc re-weighting戦略
- 追加の訓練や監督なしで既存attention weightsに適用可能
- **EACL 2026採択**

## 技術詳細

### 2つの主要な制限への対処

1. **語彙バイアス**: クエリとの語彙的類似性を過度に重視
2. **attention過集中**: 少数のトークンにattentionが集中

### ReAttnの設計

1. **Cross-document IDF weighting**: 
   - 候補文書間で頻出するクエリ重複トークンへのattentionを下げる
   - 語彙バイアスを減らし特徴的なターム強調

2. **Entropy-based regularization**:
   - 過度に集中したattentionを緩和
   - 情報的トークン間でよりバランスの取れた分布を促進

### 利点
- 追加訓練・監督不要
- 任意のattention-based re-rankerに適用可能

## 実験結果

- 広範な実験で手法の有効性を実証

## 選定理由

**EACL 2026採択**、Jian-Yun Nie（Montreal、IR著名研究者）グループ。LLM re-rankingの実用的改善。
