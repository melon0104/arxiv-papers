---
layout: paper
arxiv_id: "2602.20089"
title: "StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues"
authors: "Zanxi Ruan, Qiuyu Kong, Songqun Gao, Yiming Wang, Marco Cristani"
categories: "cs.CV cs.AI"
published: "2026-02-23"
venue: "CVPR 2026"
github: "https://github.com/intelligolabs/StructXLIP"
abstract: |
  Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions.
---

# StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues

## 概要

**エッジベース表現**をvision-languageアライメントに拡張。構造的手がかりを分離・整合させることでクロスモーダル検索を改善。

## 主な貢献

- **StructXLIP**: 構造中心のファインチューニングアライメントパラダイム
- 標準CLIPを超えるクロスモーダル検索性能
- **CVPR 2026採択**、**GitHub公開**

## 技術詳細

### 手法の核心
1. **エッジマップ抽出**（例：Canny）を画像の視覚構造のプロキシとして利用
2. 対応するキャプションをフィルタして構造的手がかりを強調（「構造中心」化）

### 3つの構造中心損失
1. エッジマップと構造テキストのアライメント
2. ローカルエッジ領域とテキストチャンクのマッチング
3. エッジマップとカラー画像の接続（表現ドリフト防止）

### 理論的観点
- 標準CLIPは視覚・テキスト埋め込み間の相互情報量を最大化
- StructXLIPは追加でマルチモーダル構造表現間の相互情報量を最大化
- この補助的最適化は本質的により困難 → より堅牢で意味的に安定な最小点へ誘導

## 実験結果
- 一般・専門ドメイン両方のクロスモーダル検索で既存手法を凌駕
- プラグアンドプレイで将来手法に統合可能

## 選定理由

**CVPR 2026採択**、**GitHub公開**。CLIPの改善手法として汎用性が高く、vision-language研究への貢献。
