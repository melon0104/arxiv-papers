---
layout: paper
arxiv_id: "2602.20102"
title: "BarrierSteer: LLM Safety via Learning Barrier Steering"
authors: "Thanh Q. Tran, Arun Verma, Kiwan Wong, Bryan Kian Hsiang Low, Daniela Rus, Wei Xiao"
categories: "cs.LG cs.AI"
published: "2026-02-23"
venue: null
github: null
abstract: |
  Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings.
---

# BarrierSteer: LLM Safety via Learning Barrier Steering

## 概要

**制御バリア関数（CBF）**を用いてLLMの安全性を推論時に強制するフレームワーク。MIT Daniela Rusグループからの貢献。

## 主な貢献

- **BarrierSteer**: 学習された非線形安全制約をモデルの潜在表現空間に埋め込み
- 推論中に不安全な応答軌跡を効率的に検出・防止
- **LLMパラメータを変更せずに**安全性を強制

## 技術詳細

### フレームワーク設計
1. **Control Barrier Functions (CBF)**: 潜在空間でのステアリング機構
2. **複数安全制約の効率的マージ**: 計算効率を維持
3. **モデル能力の保持**: 元のLLMパラメータは変更なし

### 理論的貢献
- 潜在空間でのCBF適用が原理的・計算効率的であることを理論的に確立

### 実験結果
- 敵対的成功率を大幅削減
- 不安全生成を減少
- 既存手法を凌駕

## 選定理由

**MIT Daniela Rus**（ロボティクス・AI著名研究者）グループ。LLM安全性への制御理論的アプローチ。実用的でモデルに依存しない安全性強制手法。
