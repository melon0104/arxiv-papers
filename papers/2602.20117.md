---
layout: paper
arxiv_id: "2602.20117"
title: "ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models"
authors: "Andre He, Nathaniel Weir, Kaj Bostrom, Allen Nie, Darion Cassel, Sam Bayless, Huzefa Rangwala"
categories: "cs.AI cs.LG"
published: "2026-02-23"
venue: null
github: null
abstract: |
  Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric.
---

# ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models

## 概要

**推論モデル向け合成環境の自律的スケーリング**パイプライン。RLVR（検証可能報酬によるRL）を多様な推論タスクに拡張。

## 主な貢献

- **ReSyn**: インスタンス生成器と検証器を備えた多様な推論環境を生成
- 制約充足、アルゴリズムパズル、空間推論タスクをカバー
- **BBEHベンチマークで27%相対改善**

## 技術詳細

### 背景
- RLVR：検証器からの監督でReasoning LM (RLM)を訓練
- 検証器実装は多くのタスクで解注釈より容易
- 既存手法は解中心or手作り手続的環境に依存

### ReSynの設計
1. **自律的環境生成**: 多様な推論環境を自動生成
2. **インスタンス生成器**: 各タスクのインスタンスを生成
3. **検証器**: 解の正否を検証

### 実験結果
- Qwen2.5-7B-InstructをReSynデータでRL訓練
- 推論ベンチマーク全体で一貫した改善
- **BBEHで27%相対改善**（挑戦的ベンチマーク）
- ドメイン外の数学ベンチマークでも改善

### アブレーション
- 検証器ベースの監督と**タスク多様性の両方**が有意に貢献

## 選定理由

推論環境のスケーリングによるRLM能力向上の実証的証拠。27%改善という高い効果。Amazon関連研究。
