---
layout: paper
title: "MIMIC: Inner Speech as Behavior Guides for Human-AI Coordination"
arxiv_id: "2602.20517"
date: 2026-02-26
categories: [cs.AI, cs.CL, cs.LG]
significance: "NeurIPS 2025 Spotlight"
---

# MIMIC: Inner Speech as Behavior Guides

## 基本情報
- **arXiv**: [2602.20517](https://arxiv.org/abs/2602.20517)
- **カテゴリ**: cs.AI, cs.CL, cs.LG
- **採択**: NeurIPS 2025 Spotlight

## 概要
人間の認知プロセス（内的言語が行動選択を導く）に着想を得たフレームワーク「MIMIC」を提案。言語を行動意図の内部表現として使用し、多様な人間らしい行動の模倣学習を実現。

## 主要な貢献
- Vision-Language Modelを言語的足場として活用し、観察から内的言語を生成する条件付きVAEを訓練
- 拡散ベースの行動クローニングポリシーで、現在の観察と生成された内的言語に条件付けて行動を選択
- 推論時に行動特化言語で条件付けることで、細粒度の行動ステアリングを実現
- ロボット操作タスクと人間-AI協調ゲームで、行動多様性と人間デモへの忠実度を大幅改善

## 技術的詳細
- VLMによる言語スキャフォールディング
- 条件付き変分オートエンコーダー（CVAE）で内的言語生成
- Diffusion-based behavior cloning policy

## リソース
- プロジェクトページ: https://mimic-research.github.io
- コード・事前学習済みエージェント公開

## 注目ポイント
- **NeurIPS 2025 Spotlight採択**
- 認知科学の知見をAIに応用した独創的アプローチ
- 追加デモなしで行動ステアリング可能
