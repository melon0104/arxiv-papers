---
layout: default
title: "pMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation"
---

# pMoE: 多様なエキスパートを組み合わせるMixture-of-Expertsプロンプトチューニング

[arXiv:2602.22938](https://arxiv.org/abs/2602.22938)

## 概要
Parameter-efficient fine-tuningは分類・セグメンテーション等の視覚適応タスクで有望な結果を示している。従来のプロンプトチューニングは単一の事前学習モデル（汎用または医療特化）からの知識を活用するが、同一チューニングプロセス内で多様なドメイン知識を統合するシナジーを見落としている。

## 提案手法: pMoE
Mixture-of-Expertsプロンプトチューニング手法：
- **エキスパート特化プロンプトトークン**: 各ドメインエキスパート専用のプロンプト
- **学習可能ディスパッチャー**: 動的トークンディスパッチメカニズム
- **動的貢献最適化**: 適応フェーズで各ドメインエキスパートの貢献を最適化

## 実験結果
- **47の適応タスク**（一般・医療ドメインの分類・セグメンテーション）で評価
- 大幅な性能改善を達成
- 計算効率と適応効果の最適なトレードオフを提供

## 技術的意義
- 複数ドメイン知識の効果的統合
- プロンプトレイヤーごとの動的ディスパッチ
- 単一モデルフレームワーク内での多様なエキスパート活用

## 選定理由
- 視覚適応タスクでの広範な評価
- 医療・一般ドメイン双方での有効性
