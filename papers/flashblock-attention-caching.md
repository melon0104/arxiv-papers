# FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion

- **arXiv**: [2602.05305](https://arxiv.org/abs/2602.05305)
- **カテゴリ**: cs.CV, cs.AI, cs.CL
- **投稿日**: 2026-02-05
- **プロジェクト**: https://caesarhhh.github.io/FlashBlock/

## 概要

ブロック拡散モデルの長コンテキスト設定における計算オーバーヘッドを削減する「FlashBlock」を提案。ブロック外アテンションのステップ間冗長性を活用。

## 発見

ブロック拡散の未探索の特性を発見:
- **ブロック外アテンション**: 拡散ステップ間で安定
- **ブロック内アテンション**: 大きく変動

## 主要な貢献

- **キャッシュ付きブロック外アテンション**: 安定したアテンション出力を再利用
- 拡散プロセスを変更せずにアテンション計算とKVキャッシュアクセスを削減
- スパースアテンションと直交的に組み合わせ可能

## 実験結果

- 拡散言語モデルと動画生成で評価
- トークンスループット: 最大**1.44倍向上**
- アテンション時間: 最大**1.6倍削減**
- 生成品質への影響は無視できるレベル

## リンク

- [PDF](https://arxiv.org/pdf/2602.05305)
- [プロジェクト](https://caesarhhh.github.io/FlashBlock/)
