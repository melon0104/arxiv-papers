# FOCUS: Efficient Inference for Diffusion LLMs

**arXiv**: [2601.23278](https://arxiv.org/abs/2601.23278)  
**カテゴリ**: cs.LG, cs.AR, cs.CL  
**コード**: [GitHub](https://github.com/sands-lab/FOCUS)  

## 概要

Diffusion Large Language Models (DLLMs)のデコーディング効率を大幅に改善する推論システム。

## 問題設定

DLLMのデコーディングの非効率性:
- トークンブロック全体で計算が並列化されるが、各拡散ステップでデコード可能なのは一部のみ
- 大部分の計算がデコード不可能なトークンに浪費される

**発見**: アテンション由来のトークン重要度とトークンごとのデコード確率に強い相関

## FOCUSの手法

- **動的計算集中**: デコード可能なトークンに計算を動的に集中
- **オンザフライ除去**: デコード不可能なトークンをリアルタイムで除外
- **実効バッチサイズ増加**: 計算制約を緩和しスループットをスケーラブルに

## 実験結果

- **スループット**: LMDeployと比較して最大3.52倍の改善
- **生成品質**: 複数ベンチマークで品質を維持または向上

## 一言

**アテンション重要度でデコード可能トークンを特定し、DLLMのスループットを3.5倍に**
