---
layout: paper
title: "線形表現仮説における特徴格納容量の理論的限界"
---

# How Many Features Can a Language Model Store Under the Linear Representation Hypothesis?

## 基本情報
- **arXiv**: https://arxiv.org/abs/2602.11246
- **カテゴリ**: cs.LG, cs.AI, cs.CL, cs.IT, math.CO
- **著者**: Kenny Peng et al.
- **投稿日**: 2026-02-11

## 選定理由
✅ **理論的貢献**: LLMの表現容量に関する厳密な数学的解析

## 概要
線形表現仮説（LRH）の数学的フレームワークを導入。仮説を「線形表現」（特徴がニューロン活性化に線形埋め込み）と「線形アクセス可能性」（特徴が線形デコード可能）の2つの主張に分離。

## 理論的貢献
- **上下限の確立**: k-スパース入力に対し、d = Ω(k²/log k · log(m/k))が必要、d = O(k² log m)で十分
- **古典vs線形圧縮センシングのギャップ**: 線形デコード要件により量的ギャップが存在
- **スーパーポジション仮説の理論的根拠**: ニューロンは指数的な数の特徴を格納可能

## 証明手法
- 上限: 近似直交列を持つ行列のランダム構成
- 下限: 近単位行列のランク境界とTuránの定理

## 所感
メカニスティック解釈可能性の数学的基盤を強化する重要な理論的貢献。
