# Learning Rate Scaling across LoRA Ranks and Transfer to Full Finetuning

**arXiv**: [2602.06204](https://arxiv.org/abs/2602.06204)  
**日付**: 2026-02-09  
**分野**: cs.LG（機械学習）  
**機関**: 複数大学

## 概要

LoRAにおける最適学習率とアダプタランクのスケーリング関係を理論的に特徴づけるMaximal-Update Adaptation (μA)フレームワーク。さらにLoRAからフルファインチューニングへの学習率転移を実現。

## 技術的ポイント

- **μA (Maximal-Update Adaptation)**: μPから着想を得た理論フレームワーク
- **2つのレジーム発見**: 
  1. 学習率がランクに対して不変
  2. 学習率がランクに反比例
- **LoRA → Full Finetuning転移**: 学習率チューニングコストを大幅削減

## 結果

- **言語/視覚/VLM/画像生成/RL**: 多様なタスクでスケーリング則を検証
- **学習率転移**: LoRAでチューニングした学習率がフルファインチューニングに転移
- **実用性**: 高コストなフルファインチューニングの学習率探索を回避

## 選定理由

- μP理論の重要な拡張
- 実用的なファインチューニング効率化に貢献
