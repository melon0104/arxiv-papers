# On-Policy Context Distillation (OPCD)

[📄 arXiv:2602.12275](https://arxiv.org/abs/2602.12275)

## 概要
文脈蒸留の新フレームワーク。On-policy蒸留と文脈蒸留を橋渡しし、学生モデル自身の生成軌跡上で訓練。

## 提案手法
**OPCD (On-Policy Context Distillation)**：
- 学生モデルが自己生成した軌跡上で訓練
- 文脈条件付き教師モデルとのReverse KLダイバージェンスを最小化

## 応用シナリオ
1. **経験的知識蒸留**：過去の解決軌跡から転移可能な知識を抽出・統合
2. **システムプロンプト蒸留**：最適化されたプロンプトに埋め込まれた有益な振る舞いを内在化

## 実験結果
- 数学的推論・テキストベースゲーム・ドメイン特化タスクで評価
- ベースライン手法を一貫して上回る
- タスク精度向上＋分布外能力を保持
- **クロスサイズ蒸留**：小型学生が大型教師から知識を内在化可能

## 注目ポイント
- Li Dong氏（Microsoft Research関連）
- プロンプト最適化の知識をモデルに定着させる手法

## カテゴリ
cs.CL
