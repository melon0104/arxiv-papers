---
layout: paper
title: "RL Rewriting Agent: 安定したOff-Policy SFTのためのデータ書き換え"
---

# Patch the Distribution Mismatch: RL Rewriting Agent for Stable Off-Policy SFT

## 基本情報
- **arXiv**: https://arxiv.org/abs/2602.11220
- **カテゴリ**: cs.LG, cs.CL
- **著者**: Zhongbin Guo et al.
- **投稿日**: 2026-02-11
- **GitHub**: https://anonymous.4open.science/r/Patch-the-Prompt-Gap-4112

## 選定理由
✅ **コード公開**: GitHubでコード公開
✅ **実践的課題への対処**: 破滅的忘却の緩和

## 概要
LLMを下流シナリオに適応させる際、SFTは依然として一般的だが、下流データがモデルの事前訓練分布から大きくシフトすると破滅的忘却を引き起こす可能性。データ書き換えは事前にデータを書き換えるデータ中心アプローチとして提案されてきた。

## 技術的貢献
- **ポリシー学習としてのデータ書き換え**: バックボーンのQAスタイル生成分布により適合する書き換えポリシーを学習
- **強化学習ベースエージェント**: 分布整合性、多様性、タスク一貫性を報酬フィードバックで最適化
- **3つの目的同時最適化**: QAスタイル分布整合、多様性維持、タスク一貫性ゲート

## 実験結果
- 標準SFTと同等の下流ゲインを達成
- 非下流ベンチマークでの忘却を平均12.34%削減

## 所感
分布ミスマッチ問題への実用的解決策。多様性崩壊を防ぎながら分布を整合させるRL設計が巧み。
