# Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability

## 基本情報
- **arXiv**: [2602.10067](https://arxiv.org/abs/2602.10067)
- **カテゴリ**: cs.LG
- **投稿日**: 2026-02-10

## 一言まとめ
解釈可能性の特徴量を報酬関数として使用するRLパイプライン。幻覚を**58%削減**しつつベンチマーク性能を維持。

## 概要
大規模データで訓練された言語モデルは、事実性や意図などの抽象概念をエンコードする特徴を学習。これらは従来テスト時の監視やステアリングに使用。本研究は代替用途として、オープンエンドタスクのスケーラブルな監督に使用。

## 技術的貢献
- **RLFR (Reinforcement Learning from Feature Rewards)**: 特徴量を報酬関数として使用するRLパイプライン
- **新規プロービングフレームワーク**: 幻覚候補クレームを特定
- **自己修正学習**: 事実性に不確実な場合に介入・修正を学習
- **スケーラブルなテスト時計算**: 報酬特徴でガイドされた計算スケーリング

## 実験結果
- Gemma-3-12B-ITで幻覚**58%削減**
- 標準ベンチマークでの性能維持
- 特徴の言語で監督を接地する新パラダイムを確立

## 選定理由
✅ 解釈可能性とRL の新しい組み合わせ
✅ 実用的な幻覚削減
