# THINKSAFE: Self-Generated Safety Alignment for Reasoning Models

**arXiv**: [2601.23143](https://arxiv.org/abs/2601.23143)  
**カテゴリ**: cs.AI  
**コード**: [GitHub](https://github.com/seanie12/ThinkSafe.git)  

## 概要

外部教師なしで推論モデルの安全性アラインメントを自己生成により復元するフレームワーク。

## 問題提起

大規模推論モデル (LRM) の課題:
- RLによる推論タスクへの過最適化がコンプライアンスを優先
- 有害プロンプトに対する脆弱性が増加
- 外部教師蒸留は分布の不一致を招き、本来の推論能力を低下

## ThinkSafeの洞察

**重要な発見**: コンプライアンスは安全機構を抑制するが、モデルは多くの場合、危害を特定する潜在的知識を保持している

## 手法

1. **軽量拒否ステアリング**: 潜在的安全知識を解放
2. **分布内安全推論トレースの自己生成**: モデルが自身の安全推論を生成
3. **自己生成応答での微調整**: 分布シフトを最小化しながら再アラインメント

## 実験結果

**DeepSeek-R1-Distill、Qwen3で評価**:
- **安全性**: 大幅に向上
- **推論能力**: 維持
- **GRPOと比較**: 同等の安全性・推論性能を大幅に低い計算コストで達成

## 一言

**自己生成の安全推論トレースで、推論モデルの安全性を効率的に復元**
