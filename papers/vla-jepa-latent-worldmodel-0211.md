# VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model

## 基本情報
- **arXiv**: [2602.10098](https://arxiv.org/abs/2602.10098)
- **カテゴリ**: cs.RO, cs.CV
- **投稿日**: 2026-02-10

## 一言まとめ
JEPAスタイルの事前学習でVLAを強化。ピクセル変化ではなくアクション関連状態遷移を学習し、汎化とロバスト性を向上。

## 概要
インターネット規模の動画でのVLA事前学習は魅力的だが、現在の潜在アクション目的はピクセル変化に固執し、外観バイアス、ノイズモーション、情報リークに脆弱。VLA-JEPAはこれらの落とし穴を設計で回避。

## 技術的貢献
- **リーケージフリー状態予測**: ターゲットエンコーダが将来フレームから潜在表現を生成、学生パスは現在の観察のみを参照
- **潜在空間予測**: ピクセル空間ではなく潜在空間で予測し、カメラモーションや無関係な背景変化にロバスト
- **シンプルな2段階レシピ**: JEPA事前学習 → アクションヘッドファインチューニング

## 実験結果
- LIBERO、LIBERO-Plus、SimplerEnvで一貫した向上
- 実世界操作タスクで汎化とロバスト性の改善
- 既存手法を上回る性能

## 選定理由
✅ JEPA理論の実用的適用
✅ 汎化とロバスト性の両立
